<!DOCTYPE html>
<html lang="zh">

<head>
            <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="">
        <meta name="author" content="">

        <title>JenI's Blog</title>

        <link rel="icon" href="https://blog.jenisec.org/theme/images/favicon.ico" type="image/x-icon">  
        <link rel="shortcut icon" href="https://blog.jenisec.org/theme/images/favicon.ico" type="image/x-icon">  


        <link href="https://blog.jenisec.org/theme/css/iconfont.css" rel="stylesheet">

        <!-- Bootstrap Core CSS -->
        <link href="https://blog.jenisec.org/theme/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom CSS -->
        <link href="https://blog.jenisec.org/theme/css/clean-blog.min.css" rel="stylesheet">

        <!-- Code highlight color scheme -->
            <link href="https://blog.jenisec.org/theme/css/code_blocks/monokai.css" rel="stylesheet">

        <!-- Custom Fonts -->
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->






			<meta property="og:locale" content="zh">
		<meta property="og:site_name" content="JenI's Blog">

	<meta property="og:type" content="article">
	<meta property="article:author" content="">
	<meta property="og:url" content="https://blog.jenisec.org/machinelearning/kNearestNeighbor.html">
	<meta property="og:title" content="机器学习 - k近邻算法">
	<meta property="og:description" content="">
	<meta property="og:image" content="https://blog.jenisec.org/../theme/images/posts/machinelearning/common-algorithms/cover.png">
	<meta property="article:published_time" content="2018-01-15 00:00:00+08:00">
</head>

<body style="background-color: #f8f8f8">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="https://blog.jenisec.org/">JenI's Blog</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                        <li><a href="https://blog.jenisec.org/category/ji-qi-xue-xi.html">机器学习</a></li>
                        <li><a href="https://blog.jenisec.org/category/ni-xiang-gong-cheng.html">逆向工程</a></li>
                        <li><a href="https://blog.jenisec.org/category/an-quan.html">安全</a></li>
                        <li><a href="https://blog.jenisec.org/category/xi-tong.html">系统</a></li>
                        <li><a href="https://blog.jenisec.org/category/ri-chang.html">日常</a></li>

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
        <header class="intro-header" style="background-image: url('../theme/images/posts/machinelearning/common-algorithms/cover.png')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-heading">
                        <h1>机器学习 - k近邻算法</h1>
                        <span class="meta">Posted by
                                <a href="https://blog.jenisec.org/author/jeni.html">JenI</a>
                             on 2018-01-15 00:00:00+08:00
                        </span>
                        
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1" style="background-color: #f8f8f8;position: relative;">
    <!-- Post Content -->
    <article style="background-color: #f8f8f8;position: relative;">
        <p><strong><em>前言</em></strong></p>
<p>在学习第一个算法之前，先来了解一下机器学习中的两个概念，监督学习(Supervised Learning)和无监督学习(Unsupervised Learning)。</p>
<p>训练数据有标签向量的为监督学习。首先，我们的手上会有一批训练样本，对于每一条训练样本，我们都已知它的各个特征值和最终结果。这种情况下，我们对这些已知结果的数据进行训练，得到一个模型，之后将只包含特征的数据传入模型进行预测，得出新数据的结果。</p>
<p>训练数据没有标签向量的为无监督学习。无监督学习与监督学习不同的地方在于我们并不会事先准备一批数据进行训练，而是直接对数据进行建模，根据样本间的关系直接得出结果。</p>
<p>举个简单的例子，监督学习就好比有人事先告诉你，红色且表面光滑的是苹果，橙色且表面粗糙的是橘子，然后拿给你一个表面粗糙的橙色水果问你它是苹果还是橘子。而无监督学习就是事先什么都不跟你说，直接扔给你一堆水果，这些水果中有红色且表面光滑的，也有橙色且表面粗糙的，然后让你把这堆水果根据外观分成两堆。</p>
<p><strong><em>k近邻算法</em></strong></p>
<p>k 近邻算法的基本原理可以参考我的上一篇文章，这里不作赘述：</p>
<blockquote>
<p><a href="https://blog.jenisec.org/machinelearning/common-algorithms.html">机器学习 - 常用算法的简单理解</a></p>
</blockquote>
<p>k 近邻算法属于监督学习算法，不过它并没有训练的过程，也就是说它每次对新数据点进行分类时都会重新计算新数据点与每个样本数据点的距离，因此计算的复杂性和空间复杂度都比较高。由于要进行大量计算，所以 k 近邻算法并不太适合用在特征维度较高的分类问题上。</p>
<p>k 近邻算法通常使用欧式距离公式和曼哈顿距离公式来计算两点间距离。</p>
<p>点 x = (x1,...,xn) 和 y = (y1,...,yn) 之间的欧氏距离为：
<div align="center"><img class="img-responsive" src="http://jenisec.nos-eastchina1.126.net/ml/knn/Euclidean.svg" alt="Euclidean" /></div></p></p>
<p>点 p1 =（x1, y1）与点 p2 =（x2, y2）间的曼哈顿距离为：
<div align="center"><img class="img-responsive" src="http://jenisec.nos-eastchina1.126.net/ml/knn/Manhattan.svg" alt="Manhattan" /></div></p></p>
<p>欧式距离测量的是两点间的直线距离。而曼哈顿距离为两点所形成的对轴产生的投影的距离总和，它又被称为城市街区距离，就好比一个出租车司机从一个十字路口开到另一个十字路口的路程总和。 如下图(引自维基百科)所示:
<div align="center"><img class="img-responsive" src="http://jenisec.nos-eastchina1.126.net/ml/knn/Manhattan_distance.png" alt="Manhattan_distance" /></div></p></p>
<p>当然，可以使用的距离公式并不局限于这两种，只是这两种作为常用距离公式被广为人知。下面使用 python 实现的 k 近邻算法是基于欧式距离公式的。</p>
<p><strong><em>k 近邻算法的 Python 实现</em></strong></p>
<p>k 近邻算法的核心就是计算距离，所以首先编写计算两点间欧式距离的函数。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">euclidean</span><span class="p">(</span><span class="n">list_a</span><span class="p">,</span><span class="n">list_b</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    说明：函数接收两个列表，转换为 array 后相减，再求其平方和，最后计算出它的平方根。</span>
<span class="sd">    返回：浮点型数字</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">array_a</span><span class="p">,</span> <span class="n">array_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">list_a</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">list_b</span><span class="p">)</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">((</span><span class="n">array_a</span><span class="o">-</span><span class="n">array_b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">distance</span>
</pre></div>


<p>有了计算距离的函数后，就可以根据计算出的距离对未知数据进行分类了。这里需要用到 operator 模块，在对邻居的种类进行排序时会用到。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">operator</span>

<span class="k">def</span> <span class="nf">kNNClassifier</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">unknown_data</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    说明：函数接收四个参数，第一个参数为已知类别的数据集(列表：[[],[],[]...])，第二个参数是已知类别数据集的标签向量，第三个参数为待分类的数据点(列表：[...])，第四个参数为邻居数量。</span>
<span class="sd">    返回：字符串</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># 判断 k 值是否过大，如果 k 值比原始数据集还大，则直接退出函数。</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_X</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;[-]Error: The k value is too large!&#39;</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># 计算待分类的数据点与已知类别数据集中各点的距离。</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">train_X</span><span class="p">:</span>
        <span class="n">distance</span> <span class="o">=</span> <span class="n">euclidean</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">unknown_data</span><span class="p">)</span>
        <span class="n">distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distance</span><span class="p">)</span>
    <span class="c1"># argsort 函数返回数组值从小到大的索引。</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
    <span class="c1"># 统计距离待分类数据点最近 k 个点的类别数量。</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">tmp</span><span class="p">[</span><span class="n">train_y</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">]]]</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">train_y</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">nerghbors_count</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="c1"># 以数量为依据进行降序排序。</span>
    <span class="n">nerghbors_count</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># 最终返回 k 个邻居中最多的类别。</span>
    <span class="k">return</span> <span class="n">nerghbors_count</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<p>现在我们编写了 k 近邻算法的分类器，可以试着使用这个分类器对未知数据进行分类了。这里我使用了十分流行的鸢尾花数据集，这份数据集共有 150 条数据，每一条数据包含 4 个特征值和 1 个目标变量。下图为每种花卉的各三条数据。
<div align="center"><img class="img-responsive" src="http://jenisec.nos-eastchina1.126.net/ml/knn/iris.png" alt="iris" /></div></p></p>
<p>我将鸢尾花数据集整理成了文本格式，有需要的同学可以直接点击下边的链接下载</p>
<blockquote>
<p><a href="http://jenisec.nos-eastchina1.126.net/ml/knn/iris">鸢尾花数据集</a></p>
</blockquote>
<p>编写函数将鸢尾花文本数据集处理为我们分类器可用的格式：</p>
<div class="highlight"><pre><span></span><span class="s s-Atom">def</span> <span class="nf">load_iris</span><span class="p">()</span><span class="s s-Atom">:</span>
    <span class="s s-Atom">&#39;&#39;&#39;</span>
<span class="s s-Atom">    说明：函数读取 iris 文件内容，处理为可用于分类器的格式。</span>
<span class="s s-Atom">    返回：列表，列表</span>
<span class="s s-Atom">    &#39;&#39;&#39;</span>
    <span class="s s-Atom">data</span><span class="p">,</span> <span class="s s-Atom">label</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="s s-Atom">with</span> <span class="nf">open</span><span class="p">(</span><span class="s s-Atom">&#39;iris&#39;</span><span class="p">,</span><span class="s s-Atom">&#39;r&#39;</span><span class="p">)</span> <span class="s s-Atom">as</span> <span class="nn">f</span><span class="p">:</span>
        <span class="s s-Atom">for</span> <span class="s s-Atom">i</span> <span class="s s-Atom">in</span> <span class="s s-Atom">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()</span><span class="s s-Atom">:</span>
            <span class="s s-Atom">i</span> <span class="o">=</span> <span class="s s-Atom">i</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="s s-Atom">&#39;\t&#39;</span><span class="p">)</span>
            <span class="s s-Atom">data</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="nf">float</span><span class="p">(</span><span class="s s-Atom">x</span><span class="p">)</span> <span class="s s-Atom">for</span> <span class="s s-Atom">x</span> <span class="s s-Atom">in</span> <span class="s s-Atom">i</span><span class="p">[:-</span><span class="mi">1</span><span class="p">]])</span>
            <span class="s s-Atom">label</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="s s-Atom">i</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="s s-Atom">return</span> <span class="s s-Atom">data</span><span class="p">,</span> <span class="s s-Atom">label</span>
</pre></div>


<p>现在尝试使用分类器对未知数据进行分类，假设我们有一个花萼长度为 5.8、花萼宽度为 2.7、花瓣长度为 4.1、花瓣宽度为 1.0 的花需要判断种类，我们将这个花的数据放入分类器进行判断，结果如下：</p>
<div class="highlight"><pre><span></span>train_X, train_y = load_iris()
unknown_data = [5.8, 2.7, 4.1, 1.0]
category = kNNClassifier(train_X, train_y, unknown_data, 3)
print category
</pre></div>


<div align="center"><img class="img-responsive" src="http://jenisec.nos-eastchina1.126.net/ml/knn/classifier_result.png" alt="classifier_result" /></div>

</p>

<p>可以看到分类器判断这朵花的种类为 Iris-versicolor。</p>
<p><strong><em>kNN 算法在 sklearn 中的使用</em></strong></p>
<p>下图为 sklearn 库中 kNN 算法的相关函数，包括了几种常用邻居搜索算法、基于 k 近邻的回归器、核密度估计、质心分类器等多个方面：
<div align="center"><img class="img-responsive" src="http://jenisec.nos-eastchina1.126.net/ml/knn/sklearn_neighbors_module.png" alt="sklearn_neighbors_module" /></div></p></p>
<p>其中对于几种常用邻居搜索算法，sklearn.neighbors 为其提供了统一的接口。通过 NearestNeighbors 中的 algorithm 关键字可以十分方便的选择使用 BallTree、KDTree 还是 Brute Force 暴力算法(BF)，这些算法各有优势，但不在本文的介绍范围内。这里由于只是想对数据进行简单分类，所以只介绍 KNeighborsClassifier() 方法，具体使用方法如下：</p>
<div class="highlight"><pre><span></span><span class="c1"># 从 sklearn 的数据集中导入 load_iris 方法，该方法用于加载鸢尾花数据集</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="c1"># 导入 k 近邻分类器</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">unknown_data</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="c1"># 加载数据集</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="c1"># 声明一个分类器实例</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="c1"># fit 方法是 sklearn 库在最初设计时提供的统一 API，分类、回归、聚类、维数约减、特征抽取、数据预处理这六大板块都包含 fit 方法</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="c1"># predict 方法只有分类、回归、聚类三个板块有。这里用作对未知数据进行结果预判</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">unknown_data</span><span class="p">])</span>
<span class="k">print</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">predict</span><span class="p">]</span>
</pre></div>


<p>KNeighborsClassifier() 函数可接受的参数有很多个：</p>
<div class="highlight"><pre><span></span><span class="n">n_neighbors</span> <span class="o">:</span> <span class="n">int</span><span class="o">,</span> <span class="n">optional</span> <span class="o">(</span><span class="k">default</span> <span class="o">=</span> <span class="mi">5</span><span class="o">)</span>

    <span class="n">weights</span> <span class="o">:</span> <span class="n">str</span> <span class="n">or</span> <span class="n">callable</span><span class="o">,</span> <span class="n">optional</span> <span class="o">(</span><span class="k">default</span> <span class="o">=</span> <span class="s1">&#39;uniform&#39;</span><span class="o">)</span>

    <span class="n">algorithm</span> <span class="o">:</span> <span class="o">{</span><span class="s1">&#39;auto&#39;</span><span class="o">,</span> <span class="s1">&#39;ball_tree&#39;</span><span class="o">,</span> <span class="s1">&#39;kd_tree&#39;</span><span class="o">,</span> <span class="s1">&#39;brute&#39;</span><span class="o">},</span> <span class="n">optional</span>

    <span class="n">leaf_size</span> <span class="o">:</span> <span class="n">int</span><span class="o">,</span> <span class="n">optional</span> <span class="o">(</span><span class="k">default</span> <span class="o">=</span> <span class="mi">30</span><span class="o">)</span>

    <span class="n">metric</span> <span class="o">:</span> <span class="n">string</span> <span class="n">or</span> <span class="n">DistanceMetric</span> <span class="n">object</span> <span class="o">(</span><span class="k">default</span> <span class="o">=</span> <span class="s1">&#39;minkowski&#39;</span><span class="o">)</span>

    <span class="n">p</span> <span class="o">:</span> <span class="n">integer</span><span class="o">,</span> <span class="n">optional</span> <span class="o">(</span><span class="k">default</span> <span class="o">=</span> <span class="mi">2</span><span class="o">)</span>

    <span class="n">metric_params</span> <span class="o">:</span> <span class="n">dict</span><span class="o">,</span> <span class="n">optional</span> <span class="o">(</span><span class="k">default</span> <span class="o">=</span> <span class="n">None</span><span class="o">)</span>

    <span class="n">n_jobs</span> <span class="o">:</span> <span class="n">int</span><span class="o">,</span> <span class="n">optional</span> <span class="o">(</span><span class="k">default</span> <span class="o">=</span> <span class="mi">1</span><span class="o">)</span>
</pre></div>


<p>n_neighbors 是指要搜素的最近邻数量，默认为 5；weights 是对邻居权重的配置；algorithm 是决定使用哪种近邻算法的参数，默认是自动，即分类器会从这些算法中试图找到对训练集最合适的算法；leaf_size 是叶片大小，只有在使用 BallTree 和 KDTree 时这个参数才有意义，默认 30；metric 参数的默认值是 minkowski，即闵式距离。实际上闵式距离并不是一种距离，而是一组距离的定义，可以把他当作是距离公式的选择器，它与下一个参数 p 相关，当 p=1 时，使用曼哈顿距离，p=2 时，使用标准欧氏距离，当 p&gt;2 时，就是切比雪夫距离；还有一个 n_jobs 参数，这个参数表示使用多少个 CPU 核并行搜索，默认为 1。这里我们全部使用默认值，运行结果和我们自己编写的分类器结果一致：
<div align="center"><img class="img-responsive" src="http://jenisec.nos-eastchina1.126.net/ml/knn/sklearn_classifier_result.png" alt="sklearn_classifier_result" /></div></p></p>
<p><strong><em>参考</em></strong></p>
<blockquote>
<p><a href="http://scikit-learn.org/dev/modules/classes.html#module-sklearn.neighbors">http://scikit-learn.org/dev/modules/classes.html#module-sklearn.neighbors</a>
<a href="http://scikit-learn.org/dev/modules/neighbors.html#neighbors">http://scikit-learn.org/dev/modules/neighbors.html#neighbors</a></p>
</blockquote>
        <hr />
        <p>作者:&#160; &#160;JenI&#160; &#160;转载请注明出处，谢谢</p>
    </article>

    <hr>

        <div class="comments">
            <h2>Comments !</h2>
            <div id="disqus_thread"></div>
            <script type="text/javascript">
                var disqus_shortname = 'jenihk';
                var disqus_identifier = 'machinelearning/kNearestNeighbor.html';
                var disqus_url = 'https://blog.jenisec.org/machinelearning/kNearestNeighbor.html';
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//jenihk.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            </script>
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
 <!--                   
    <div id="disqus_thread"></div>
    <script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = '//jenihk.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->
            </div>
        </div>
    </div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="http://weibo.com/1992014191/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1">
                                <span class="fa-stack fa-lg">
                                    <i class="icon iconfont">&#xe657;</i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://github.com/jenihk">
                                <span class="fa-stack fa-lg">
                                    <i class="icon iconfont">&#xe618;</i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <p class="copyright text-muted">Blog powered by <a href="http://getpelican.com">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="https://blog.jenisec.org/theme/js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="https://blog.jenisec.org/theme/js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="https://blog.jenisec.org/theme/js/clean-blog.min.js"></script>
    
    <!-- CanvasParticles -->
    <script src="https://blog.jenisec.org/theme/js/canvas-particle.js"></script>

    

    <script type="text/javascript">
        CanvasParticle();
    </script>

    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-85780345-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script type="text/javascript">
    var disqus_shortname = 'jenihk';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>

</html>